<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>AI Attractiveness Rater (Simple & Reliable)</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Use TFJS 3.x for best compatibility with face-api.js -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2"></script>
<style>
  body { font-family: Arial, sans-serif; text-align: center; background: #f7f7f7; padding: 20px; color:#222; }
  h1 { margin: 10px 0 20px; }
  .row { margin: 10px 0; }
  button, input[type=file] { font-size: 16px; padding: 10px 14px; margin: 8px; }
  video, img { max-width: 95%; border-radius: 10px; margin-top: 12px; }
  #score { font-size: 28px; font-weight: 700; margin-top: 18px; }
  .loader { border: 6px solid #eee; border-top: 6px solid #3498db; border-radius: 50%; width: 42px; height: 42px; animation: spin 1s linear infinite; margin: 18px auto; display: none; }
  @keyframes spin { 0% { transform: rotate(0deg);} 100% { transform: rotate(360deg);} }
  #note { font-size: 12px; color:#666; margin-top:10px; }
  #error { color:#c00; margin-top:10px; display:none; }
</style>
</head>
<body>

<h1>AI Attractiveness Rater</h1>

<div class="row">
  <input type="file" id="fileInput" accept="image/*">
  <button id="cameraBtn">Use Camera</button>
  <button id="snapBtn" style="display:none;">Snap Photo</button>
</div>

<video id="cameraStream" autoplay playsinline muted style="display:none;"></video>
<canvas id="snapshotCanvas" style="display:none;"></canvas>

<div class="loader" id="loader"></div>
<img id="preview" style="display:none;" alt="Your photo preview">
<div id="score"></div>
<div id="error"></div>
<div id="note">Tip: Good lighting and a clear face shot improve detection.</div>

<script>
let beautyModel, stream;
let tinyOptions = new faceapi.TinyFaceDetectorOptions({ inputSize: 320, scoreThreshold: 0.5 });

/* --- Robust model loading --- */
(async function init() {
  try {
    document.getElementById("loader").style.display = "block";

    // Pick a reliable TF backend
    try {
      await tf.setBackend('webgl');
    } catch(e) {
      await tf.setBackend('cpu');
    }
    await tf.ready();

    // Load face detector (tiny, fast, mobile-friendly)
    await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/models');

    // Load beauty model (returns ~0-10)
    beautyModel = await tf.loadLayersModel('https://huggingface.co/datasets/ayasy/face_beauty_tfjs/resolve/main/model.json');

    document.getElementById("loader").style.display = "none";
  } catch(err) {
    fail("Failed to load models. Please refresh.");
    console.error(err);
  }
})();

/* --- Helpers --- */
function fail(msg) {
  document.getElementById("loader").style.display = "none";
  const err = document.getElementById("error");
  err.style.display = "block";
  err.textContent = msg;
}
function clearStatus() {
  document.getElementById("score").textContent = "";
  const err = document.getElementById("error");
  err.style.display = "none";
  err.textContent = "";
}

/* --- Scoring Pipeline --- */
async function scoreImageFromHTMLElement(imgEl) {
  clearStatus();
  document.getElementById("loader").style.display = "block";

  try {
    // Try face detection first
    let det = await faceapi.detectSingleFace(imgEl, tinyOptions);

    // Create a square crop canvas the model expects (224x224)
    const cropSize = 224;
    const faceCanvas = document.createElement('canvas');
    faceCanvas.width = cropSize;
    faceCanvas.height = cropSize;
    const ctx = faceCanvas.getContext('2d');

    if (det) {
      // Crop around detected face with a little padding
      const pad = 0.25;
      const { x, y, width, height } = det.box;

      const sx = Math.max(0, x - width * pad);
      const sy = Math.max(0, y - height * pad);
      const sw = Math.min(imgEl.width || imgEl.videoWidth, width * (1 + 2*pad));
      const sh = Math.min(imgEl.height || imgEl.videoHeight, height * (1 + 2*pad));

      ctx.drawImage(imgEl, sx, sy, sw, sh, 0, 0, cropSize, cropSize);
    } else {
      // Fallback: center-crop the image so the user still gets a score
      const srcW = imgEl.width || imgEl.videoWidth;
      const srcH = imgEl.height || imgEl.videoHeight;
      if (!srcW || !srcH) {
        throw new Error("Could not read image dimensions.");
      }
      const side = Math.min(srcW, srcH);
      const sx = (srcW - side) / 2;
      const sy = (srcH - side) / 2;
      ctx.drawImage(imgEl, sx, sy, side, side, 0, 0, cropSize, cropSize);
    }

    // Normalize and predict
    const tensor = tf.tidy(() => tf.browser.fromPixels(faceCanvas).toFloat().div(255).expandDims(0));
    const pred = beautyModel.predict(tensor);
    const data = await pred.data();
    const rawScore = data[0];
    pred.dispose();
    tensor.dispose();

    // Clamp and format score (1..10)
    const finalScore = Math.max(1, Math.min(10, rawScore)).toFixed(1);

    document.getElementById("loader").style.display = "none";
    document.getElementById("score").textContent = `Score: ${finalScore} / 10`;
  } catch (err) {
    console.error(err);
    fail("Sorry, couldn't score that image. Try a clearer face photo.");
  }
}

/* --- File upload flow --- */
document.getElementById('fileInput').addEventListener('change', async function() {
  const file = this.files && this.files[0];
  if (!file) return;
  clearStatus();

  try {
    const img = await faceapi.bufferToImage(file);   // gives HTMLImageElement
    const preview = document.getElementById('preview');
    preview.src = URL.createObjectURL(file);
    preview.style.display = 'block';
    await scoreImageFromHTMLElement(img);
  } catch (err) {
    console.error(err);
    fail("Could not read that image file.");
  }
});

/* --- Camera capture flow --- */
document.getElementById('cameraBtn').addEventListener('click', async () => {
  clearStatus();
  try {
    // Request camera
    stream = await navigator.mediaDevices.getUserMedia({ video: true });
    const video = document.getElementById('cameraStream');
    video.srcObject = stream;
    video.style.display = 'block';

    // Wait until we have real dimensions to draw from
    await new Promise(resolve => {
      if (video.readyState >= 2) return resolve();
      video.onloadedmetadata = resolve;
    });

    // Show snap button now that video is ready
    document.getElementById('snapBtn').style.display = 'inline-block';
  } catch (err) {
    console.error(err);
    fail("Camera access denied or unavailable.");
  }
});

document.getElementById('snapBtn').addEventListener('click', async () => {
  const video = document.getElementById('cameraStream');
  const canvas = document.getElementById('snapshotCanvas');

  // Guard against zero-size video (rare timing issue)
  if (!video.videoWidth || !video.videoHeight) {
    fail("Camera not ready yet. Wait a second and try again.");
    return;
  }

  // Draw current frame to canvas
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
  const ctx2 = canvas.getContext('2d');
  ctx2.drawImage(video, 0, 0);

  // Stop camera
  try { stream.getTracks().forEach(t => t.stop()); } catch(e){}

  // Convert canvas to an Image element that face-api can handle
  const dataURL = canvas.toDataURL('image/png');
  const img = new Image();
  img.onload = async () => {
    // Show preview
    const preview = document.getElementById('preview');
    preview.src = dataURL;
    preview.style.display = 'block';

    // Hide camera UI
    document.getElementById('cameraStream').style.display = 'none';
    document.getElementById('snapBtn').style.display = 'none';

    // Score it
    await scoreImageFromHTMLElement(img);
  };
  img.onerror = () => fail("Couldn't read the camera snapshot. Try again.");
  img.src = dataURL;
});
</script>

</body>
</html>